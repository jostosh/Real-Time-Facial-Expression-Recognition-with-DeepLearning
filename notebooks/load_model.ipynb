{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_estimator = load_model('/home/demo/anchormen/emotion-rec/model/age.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layer_dict = {layer.name: layer for layer in age_estimator.layers}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'conv2d_2': <keras.layers.convolutional.Conv2D object at 0x7fdf71816a90>, 'dropout_2': <keras.layers.core.Dropout object at 0x7fdf7169c908>, 'conv2d_1': <keras.layers.convolutional.Conv2D object at 0x7fdf71816ef0>, 'conv2d_3': <keras.layers.convolutional.Conv2D object at 0x7fdf71815400>, 'dense_1': <keras.layers.core.Dense object at 0x7fdf716ce630>, 'dropout_1': <keras.layers.core.Dropout object at 0x7fdf7167c4e0>, 'max_pooling2d_1': <keras.layers.pooling.MaxPooling2D object at 0x7fdf71816400>, 'conv2d_5': <keras.layers.convolutional.Conv2D object at 0x7fdf7181ba58>, 'max_pooling2d_2': <keras.layers.pooling.MaxPooling2D object at 0x7fdf7183bba8>, 'dense_2': <keras.layers.core.Dense object at 0x7fdf71694d30>, 'conv2d_4': <keras.layers.convolutional.Conv2D object at 0x7fdf7184f470>, 'conv2d_6': <keras.layers.convolutional.Conv2D object at 0x7fdf718a1ba8>, 'flatten_1': <keras.layers.core.Flatten object at 0x7fdf7167c630>, 'max_pooling2d_3': <keras.layers.pooling.MaxPooling2D object at 0x7fdf71847978>, 'dense_3': <keras.layers.core.Dense object at 0x7fdf71658cc0>}\n"
     ]
    }
   ],
   "source": [
    "print(layer_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv2d_2 = layer_dict['conv2d_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'conv2d_2/Elu:0' shape=(?, 26, 26, 32) dtype=float32>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d_2.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "im = cv2.imread('/home/demo/anchormen/emotion-rec/data/aligned/7153718@N04/landmark_aligned_face.2282.11597961815_4916cbf003_o.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(816, 816, 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:bde]",
   "language": "python",
   "name": "conda-env-bde-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
